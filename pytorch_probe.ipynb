{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attempt using modifed linear probe and training loop from othello_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# taken from othello_world repo\n",
    "class LinearProbe(nn.Module):\n",
    "    # num task is just for folding multiple probes into one\n",
    "    # probe class is number of possible classes to predict\n",
    "    def __init__(self, device, probe_class, num_task, input_dim=512):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.probe_class = probe_class\n",
    "        self.num_task = num_task\n",
    "        self.proj = nn.Linear(self.input_dim, self.probe_class * self.num_task, bias=True)\n",
    "        self.apply(self._init_weights)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, act, y=None):\n",
    "        # [B, f], [B, #task]\n",
    "        logits = self.proj(act).reshape(-1, self.num_task, self.probe_class)  # [B, #task, C]\n",
    "        if y is None:\n",
    "            return logits, None\n",
    "        else:\n",
    "            targets = y.to(torch.long)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-100)\n",
    "            return logits, loss\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                if pn.endswith('bias'):\n",
    "                    # biases of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        # no_decay.add('pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "        print(\"Decayed:\", decay)\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.Adam(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.75, patience=0)\n",
    "        return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple training loop; Boilerplate that could apply to any arbitrary neural network,\n",
    "so nothing in this file really has anything to do with GPT specifically.\n",
    "\"\"\"\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_dataset, test_dataset, config):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        if not os.path.exists(self.config.ckpt_path):\n",
    "            os.makedirs(self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), os.path.join(self.config.ckpt_path, \"checkpoint.ckpt\"))\n",
    "\n",
    "    def train(self, prt=True):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer, scheduler = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            data = self.train_dataset if is_train else self.test_dataset\n",
    "            loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
    "                                batch_size=config.batch_size,\n",
    "                                num_workers=config.num_workers)\n",
    "\n",
    "            losses = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader), disable=not prt) if is_train else enumerate(loader)\n",
    "            for it, (x, y) in pbar:\n",
    "                x = x.to(self.device)  # [B, f]\n",
    "                y = y.to(self.device)  # [B, #task] \n",
    "\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                    y_hat = torch.argmax(logits, dim=-1, keepdim=False)  # [B, #task]\n",
    "                    hits = y_hat == y  # [B, #task]\n",
    "\n",
    "                if is_train:\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "                    mean_loss = float(np.mean(losses))\n",
    "                    # mean_acc = np.sum(hits_epoch).item() / np.sum(totals_epoch).item()\n",
    "                    mean_acc = hits.sum().item() / hits.numel()\n",
    "                    lr = optimizer.param_groups[0]['lr']\n",
    "                    pbar.set_description(f\"epoch {epoch+1}: train loss {mean_loss:.5f}; lr {lr:.2e}; train acc {mean_acc*100:.2f}%\")\n",
    "                    \n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                scheduler.step(test_loss)\n",
    "                # test_acc = np.sum(hits_epoch).item() / np.sum(totals_epoch).item()\n",
    "                test_acc = hits.sum().item() / hits.numel()\n",
    "                if prt: \n",
    "                    logger.info(f\"test loss {test_loss:.5f}; test acc {test_acc*100:.2f}%\")\n",
    "                return test_loss\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        self.tokens = 0  # counter used for learning rate decay\n",
    "        \n",
    "        for epoch in range(config.max_epochs):\n",
    "            run_epoch('train')\n",
    "            if self.test_dataset is not None:\n",
    "                test_loss = run_epoch('test')\n",
    "                if test_loss < best_loss:\n",
    "                    best_loss = test_loss\n",
    "                    self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbingDataset(Dataset):\n",
    "    def __init__(self, act, y):\n",
    "        assert len(act) == len(y)\n",
    "        print(f\"dataset: {len(act)} pairs loaded...\")\n",
    "        self.act = act\n",
    "        self.y = y\n",
    "        print(\"y:\", np.unique(y, return_counts=True))\n",
    "        \n",
    "    def __len__(self, ):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.act[idx]), torch.tensor(self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded act: (63820, 768)\n",
      "Loaded labels: (63820,)\n",
      "dataset: 63820 pairs loaded...\n",
      "y: (array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]), array([6343, 6350, 6445, 6423, 6355, 6327, 6444, 6378, 6414, 6341]))\n"
     ]
    }
   ],
   "source": [
    "LAYER = 2\n",
    "\n",
    "act = np.load('63k_X_alllayers.npy')\n",
    "labels = np.load('63k_Y.npy')\n",
    "\n",
    "act = act[LAYER, :, :]\n",
    "\n",
    "print(f\"Loaded act: {act.shape}\")\n",
    "print(f\"Loaded labels: {labels.shape}\")\n",
    "\n",
    "probing_dataset = ProbingDataset(act, labels)\n",
    "train_size = int(0.8 * len(probing_dataset))\n",
    "test_size = len(probing_dataset) - train_size\n",
    "probe_train_dataset, probe_test_dataset = torch.utils.data.random_split(probing_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = LinearProbe(device, 10, 1, input_dim=act.shape[1])\n",
    "\n",
    "max_epochs = 10\n",
    "tconf = TrainerConfig(\n",
    "    max_epochs=max_epochs, \n",
    "    batch_size=1024, \n",
    "    learning_rate=1e-3,\n",
    "    betas=(.9, .999), \n",
    "    lr_decay=True, \n",
    "    warmup_tokens=len(probe_train_dataset)*5, \n",
    "    final_tokens=len(probe_test_dataset)*max_epochs,\n",
    "    num_workers=0, \n",
    "    weight_decay=0., \n",
    "    ckpt_path=os.path.join(f\"./ckpts/testprobe_layer{LAYER}\")\n",
    ")\n",
    "trainer = Trainer(probe, probe_train_dataset, probe_test_dataset, tconf)\n",
    "trainer.train(prt=True)\n",
    "trainer.save_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## super simple linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbingDataset(Dataset):\n",
    "    def __init__(self, act, y):\n",
    "        assert len(act) == len(y)\n",
    "        print(f\"dataset: {len(act)} pairs loaded...\")\n",
    "        self.act = act\n",
    "        self.y = y\n",
    "        print(\"y:\", np.unique(y, return_counts=True))\n",
    "        \n",
    "    def __len__(self, ):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.act[idx]), torch.tensor(self.y[idx]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded act: (63820, 768)\n",
      "Loaded labels: (63820,)\n",
      "dataset: 63820 pairs loaded...\n",
      "y: (array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]), array([6343, 6350, 6445, 6423, 6355, 6327, 6444, 6378, 6414, 6341]))\n",
      "split into [test/train], [12764/51056]\n"
     ]
    }
   ],
   "source": [
    "LAYER = 3\n",
    "\n",
    "act = np.load('63k_X_alllayers.npy')\n",
    "labels = np.load('63k_Y.npy')\n",
    "\n",
    "act = act[LAYER, :, :]\n",
    "\n",
    "print(f\"Loaded act: {act.shape}\")\n",
    "print(f\"Loaded labels: {labels.shape}\")\n",
    "\n",
    "probing_dataset = ProbingDataset(act, labels)\n",
    "train_size = int(0.8 * len(probing_dataset))\n",
    "test_size = len(probing_dataset) - train_size\n",
    "probe_train_dataset, probe_test_dataset = torch.utils.data.random_split(probing_dataset, [train_size, test_size])\n",
    "print(f\"split into [test/train], [{test_size}/{train_size}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.685244, Acc: 0.739521: 100%|██████████| 50/50 [00:41<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, num_input_features, num_classes):\n",
    "        super(LinearProbe, self).__init__()\n",
    "        self.linear = nn.Linear(num_input_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "probe = LinearProbe(768, 10)\n",
    "\n",
    "config = {\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-3,\n",
    "    'batch_size': 1024,\n",
    "    'num_epochs': 50,\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(probe.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "\n",
    "dataloader = DataLoader(probe_train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "# training loop\n",
    "bar = tqdm(range(config['num_epochs']))\n",
    "for epoch in bar:\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = probe(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # train accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    bar.set_description(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.6f}, Acc: {correct/total:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 38.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.68482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataloader = DataLoader(probe_test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "probe.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_dataloader):\n",
    "\n",
    "        outputs = probe(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        y_pred.append(predicted.cpu().numpy())\n",
    "\n",
    "print(f'Test Accuracy: {correct/total:.5f}')\n",
    "\n",
    "y_pred = np.concatenate(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.98      0.98      1227\n",
      "         1.0       0.93      0.93      0.93      1269\n",
      "         2.0       0.86      0.86      0.86      1350\n",
      "         3.0       0.72      0.70      0.71      1279\n",
      "         4.0       0.57      0.68      0.62      1271\n",
      "         5.0       0.57      0.48      0.52      1288\n",
      "         6.0       0.52      0.50      0.51      1303\n",
      "         7.0       0.50      0.42      0.46      1251\n",
      "         8.0       0.49      0.54      0.52      1290\n",
      "         9.0       0.70      0.77      0.74      1236\n",
      "\n",
      "    accuracy                           0.68     12764\n",
      "   macro avg       0.68      0.69      0.68     12764\n",
      "weighted avg       0.68      0.68      0.68     12764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_full = np.load('63k_Y.npy')\n",
    "print(classification_report(y_full[probe_test_dataset.indices], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
