{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c6383f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsn3254/.conda/envs/jupyter-kernel-py38/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformer_lens\n",
    "\n",
    "from probe_model import LinearProbe, Trainer, TrainerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "689541c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import probe_model\n",
    "importlib.reload(probe_model)\n",
    "from probe_model import LinearProbe, Trainer, TrainerConfig\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "05a14cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d1df1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9faf8704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device_name = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device_name = \"cuda\" # CUDA for NVIDIA GPU\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(f\"Device: {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30c33a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2-small\"\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a90e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "spacy_model = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3013b3",
   "metadata": {},
   "source": [
    "# Prewar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "32dba148",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "def generate_word_idxs(text):\n",
    "    \n",
    "    gpt_tokens = model.to_tokens(text).squeeze(0)\n",
    "    gpt_tokens_str = [model.to_single_str_token(int(t)) for t in gpt_tokens]\n",
    "    \n",
    "    doc = spacy_model(text)\n",
    "    words = [t.text for t in doc if t.is_alpha]\n",
    "    word_idxs = []\n",
    "    \n",
    "    i = 0\n",
    "    cur = 0 # current word index\n",
    "    sub_idx = 0 # sub index of current word\n",
    "    #print(gpt_tokens_str)\n",
    "    \n",
    "    if not len(words):\n",
    "        return [], []\n",
    "\n",
    "    while i < len(gpt_tokens_str):\n",
    "        t = gpt_tokens_str[i].strip()\n",
    "        # skip if token is just a newline or other whitespace\n",
    "        if not len(t):\n",
    "            word_idxs.append(-1)\n",
    "            i += 1\n",
    "            continue\n",
    "#         print(cur)\n",
    "#         print(words)\n",
    "        cur_word = words[cur]\n",
    "        # if token is part of current word, update sub_idx, continue to next token\n",
    "        if cur_word.find(t, sub_idx) != -1:\n",
    "            word_idxs.append(cur)\n",
    "            sub_idx += len(t)\n",
    "            i += 1\n",
    "        else:\n",
    "            # if token not in cur_word, check next word\n",
    "            if cur+1 < len(words) and t in words[cur+1]:\n",
    "                cur += 1\n",
    "                sub_idx = 0\n",
    "            # if not in cur_word or next word, give up and continue\n",
    "            else:\n",
    "                word_idxs.append(-1)\n",
    "                i += 1 \n",
    "\n",
    "#     print(f\"text {text}\")\n",
    "#     print(f\"gpt_tokens {gpt_tokens}\")\n",
    "#     print(f\"gpt_tokens_str {gpt_tokens_str}\")\n",
    "#     print(f\"words {words}\")\n",
    "\n",
    "    word_idxs.extend([-1]*(BATCH_SIZE - len(word_idxs)))\n",
    "    \n",
    "    gpt_tokens_padded = pad(gpt_tokens, (0, BATCH_SIZE - gpt_tokens.shape[0]))\n",
    "\n",
    "    return gpt_tokens_padded, np.array(word_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0e92006b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50256, 20342,   428,   318,   281,  1672,  6827,    11,  5145,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        device='cuda:0'),\n",
       " array([-1,  0,  1,  2,  3,  4,  5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_word_idxs(\"hey this is an example sentence, !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ccc9ae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[191], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     prewar \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tqdm(prewar[:N_LINES]):\n\u001b[0;32m---> 12\u001b[0m     tokens, word_idxs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_word_idxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     all_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tokens\n\u001b[1;32m     15\u001b[0m     all_word_idxs\u001b[38;5;241m.\u001b[39mappend(word_idxs)\n",
      "Cell \u001b[0;32mIn[177], line 4\u001b[0m, in \u001b[0;36mgenerate_word_idxs\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_word_idxs\u001b[39m(text):\n\u001b[0;32m----> 4\u001b[0m     gpt_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m     gpt_tokens_str \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mto_single_str_token(\u001b[38;5;28mint\u001b[39m(t)) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m gpt_tokens]\n\u001b[1;32m      7\u001b[0m     doc \u001b[38;5;241m=\u001b[39m spacy_model(text)\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-kernel-py38/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:767\u001b[0m, in \u001b[0;36mHookedTransformer.to_tokens\u001b[0;34m(self, input, prepend_bos, padding_side, move_to_device, truncate)\u001b[0m\n\u001b[1;32m    764\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_tokens_with_bos_removed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, tokens)\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m move_to_device:\n\u001b[0;32m--> 767\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "all_resids = []\n",
    "all_tokens = []\n",
    "all_word_idxs = []\n",
    "\n",
    "N_LINES = 500\n",
    "with open('datasets/prewar.txt', 'r') as f:\n",
    "    prewar = [line.rstrip(\"\\n\") for line in f.readlines()]\n",
    "\n",
    "for sentence in tqdm(prewar[:N_LINES]):\n",
    "    tokens, word_idxs = generate_word_idxs(sentence)\n",
    "\n",
    "    all_tokens += tokens\n",
    "    all_word_idxs.append(word_idxs)\n",
    "\n",
    "\n",
    "all_tokens = torch.stack(all_tokens)\n",
    "#print(all_tokens)\n",
    "\n",
    "all_word_idx = np.array(all_word_idx)\n",
    "\n",
    "#_, cache = model.run_with_cache(all_tokens, names_filter=lambda x: x.endswith(\"resid_post\"))\n",
    "_, cache = model.run_with_cache(all_tokens)\n",
    "residuals = cache.stack_activation(\"resid_post\")\n",
    "\n",
    "print(f\"model.cfg.d_model {model.cfg.d_model}\")\n",
    "\n",
    "# residuals contains num_layers, patch size, seq length, hidden dimension\n",
    "residuals_np = residuals.cpu().numpy()\n",
    "all_resids = residuals_np[:].reshape(12, -1, model.cfg.d_model)\n",
    "#all_resids = residuals.cpu().numpy()[:].reshape(12, -1, model.cfg.d_model)\n",
    "\n",
    "x_all_layers = all_resids\n",
    "print(all_resids.shape)\n",
    "y = np.concatenate(all_word_idxs)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0f0bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "93568ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbingDataset(Dataset):\n",
    "    def __init__(self, act, y):\n",
    "        assert len(act) == len(y)\n",
    "        print(f\"dataset: {len(act)} pairs loaded...\")\n",
    "        self.act = act\n",
    "        self.y = y\n",
    "        print(\"y:\", np.unique(y, return_counts=True))\n",
    "        \n",
    "    def __len__(self, ):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.act[idx]), torch.tensor(self.y[idx]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bf358",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER = 3\n",
    "x = x_all_layers[LAYER, :, :]\n",
    "\n",
    "probing_dataset = ProbingDataset(x, y)\n",
    "train_size = int(0.8 * len(probing_dataset))\n",
    "test_size = len(probing_dataset) - train_size\n",
    "probe_train_dataset, probe_test_dataset = torch.utils.data.random_split(probing_dataset, [train_size, test_size])\n",
    "print(f\"split into [test/train], [{test_size}/{train_size}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1ebff6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = LinearProbe(device, 768, 10)\n",
    "\n",
    "folder = f\"ckpts/{model_name}/randwords_159k/layer{LAYER}\"\n",
    "config = TrainerConfig(num_epochs=40, ckpt_path=folder)\n",
    "trainer = Trainer(device, probe, probe_train_dataset, probe_test_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56123337",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239655c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.generate_report())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-kernel-py38)",
   "language": "python",
   "name": "jupyter-kernel-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
