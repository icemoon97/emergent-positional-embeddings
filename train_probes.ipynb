{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad888284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe9b37",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b39796f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device_name = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device_name = \"cuda\" # CUDA for NVIDIA GPU\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(f\"Device: {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "062e5fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2-small\"\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf677c",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a86780",
   "metadata": {},
   "source": [
    "### Load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e20c3cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORDS = 10000\n",
    "with open('common_words.txt', 'r') as file:\n",
    "    words = np.random.choice([l.rstrip(\"\\n\") for l in file.readlines()], N_WORDS)\n",
    "    \n",
    "word_len_dict = {w: len(model.to_tokens(f\" {w}\", prepend_bos=False).squeeze(0)) for w in words}\n",
    "word_len = np.vectorize(lambda x: word_len_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe4fdbb",
   "metadata": {},
   "source": [
    "### generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a980b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"Hello and welcome to my blog, where I love to list words.\\nWhat \"\n",
    "BATCH_SIZE = 256\n",
    "N_SAMPLE = 10\n",
    "\n",
    "prefix_len = len(model.to_tokens(PREFIX, prepend_bos=True).squeeze(0)) - 1\n",
    "\n",
    "def generate_batch():\n",
    "    batch_words = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        sampled = np.random.choice(words, N_SAMPLE)\n",
    "\n",
    "        batch_words.append(sampled)\n",
    "\n",
    "    tokens = model.to_tokens([PREFIX + \" \".join(s) for s in batch_words], prepend_bos=True)\n",
    "    mapped_len = word_len(batch_words)\n",
    "\n",
    "    word_idxs = np.ones((BATCH_SIZE, N_SAMPLE * 3)) * -1\n",
    "    for i, r in enumerate(mapped_len):\n",
    "        row = np.repeat(np.arange(N_SAMPLE), r)\n",
    "        word_idxs[i, :len(row)] = row\n",
    "\n",
    "    return tokens, word_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a08d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:16<00:00,  3.07it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_BATCHES = 50\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "all_resids = []\n",
    "all_word_idxs = []\n",
    "\n",
    "for i_batch in tqdm(range(DATA_BATCHES)):\n",
    "    tokens, word_idxs = generate_batch()\n",
    "    _, cache = model.run_with_cache(tokens, names_filter=lambda x: x.endswith(\"resid_post\"))\n",
    "    residuals = cache.stack_activation(\"resid_post\")\n",
    "\n",
    "    residuals = residuals[:, :, prefix_len:, :]\n",
    "    word_idxs = word_idxs[:, :residuals.size(dim=2)]\n",
    "\n",
    "    mask = word_idxs != -1\n",
    "\n",
    "    all_resids.append(residuals.cpu().numpy()[:, mask].reshape(12, -1, model.cfg.d_model))\n",
    "    all_word_idxs.append(word_idxs[mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe6df234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 157946, 768)\n",
      "(157946,)\n"
     ]
    }
   ],
   "source": [
    "x_all_layers = np.concatenate(all_resids, axis=1)\n",
    "y = np.concatenate(all_word_idxs)\n",
    "\n",
    "print(x_all_layers.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf3ee3",
   "metadata": {},
   "source": [
    "## Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d177765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbingDataset(Dataset):\n",
    "    def __init__(self, act, y):\n",
    "        assert len(act) == len(y)\n",
    "        print(f\"dataset: {len(act)} pairs loaded...\")\n",
    "        self.act = act\n",
    "        self.y = y\n",
    "        print(\"y:\", np.unique(y, return_counts=True))\n",
    "        \n",
    "    def __len__(self, ):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.act[idx]), torch.tensor(self.y[idx]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5784815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: 157946 pairs loaded...\n",
      "y: (array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]), array([15770, 15752, 15816, 15906, 15737, 15847, 15804, 15790, 15805,\n",
      "       15719]))\n",
      "split into [test/train], [31590/126356]\n"
     ]
    }
   ],
   "source": [
    "LAYER = 3\n",
    "x = x_all_layers[LAYER, :, :]\n",
    "\n",
    "probing_dataset = ProbingDataset(x, y)\n",
    "train_size = int(0.8 * len(probing_dataset))\n",
    "test_size = len(probing_dataset) - train_size\n",
    "probe_train_dataset, probe_test_dataset = torch.utils.data.random_split(probing_dataset, [train_size, test_size])\n",
    "print(f\"split into [test/train], [{test_size}/{train_size}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6db8f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, num_input_features, num_classes):\n",
    "        super(LinearProbe, self).__init__()\n",
    "        self.linear = nn.Linear(num_input_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ea0f68e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.conda/envs/take2/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/take2/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "probe = LinearProbe(768, 10).to(device)\n",
    "\n",
    "config = {\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-3,\n",
    "    'batch_size': 1024,\n",
    "    'num_epochs': 50,\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(probe.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "\n",
    "dataloader = DataLoader(probe_train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "probe.train(True)\n",
    "\n",
    "# simple training loop\n",
    "bar = tqdm(range(config['num_epochs']))\n",
    "for epoch in bar:\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = probe(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # train accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    bar.set_description(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.6f}, Acc: {correct/total:.6f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transformer_lens)",
   "language": "python",
   "name": "take2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
